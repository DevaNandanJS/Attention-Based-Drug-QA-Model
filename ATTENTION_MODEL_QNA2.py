# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLnkpevzrZFzXMWQJWBpknKRsswwpc7U
"""

# Install necessary libraries
!pip install transformers
!pip install torch

# Optionally, install additional packages like pandas if not present:
!pip install pandas

from google.colab import files
import pandas as pd

# Upload the CSV files. You will be prompted to select your files.
uploaded = files.upload()

# Load the CSV files. Adjust the file names if necessary.
df_bio = pd.read_csv('biology_202004.csv')
df_med = pd.read_csv('medicalsciences_202004.csv')
df_nut = pd.read_csv('nutrition_202004.csv')

# Optionally, add a 'domain' column to identify the source.
df_bio['domain'] = 'biology'
df_med['domain'] = 'medicalsciences'
df_nut['domain'] = 'nutrition'

# Combine the CSV files into one DataFrame.
df_all = pd.concat([df_bio, df_med, df_nut], ignore_index=True)
print("Combined dataset shape:", df_all.shape)
print(df_all.head())

from transformers import AutoTokenizer

# Select a biomedical pretrained model. You can choose BioBERT or PubMedBERT.
model_name = 'dmis-lab/biobert-base-cased-v1.1'
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define a custom dataset
from torch.utils.data import Dataset

class BiQADataset(Dataset):
    def __init__(self, df, tokenizer, max_length=128):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Modify these column names if yours differ.
        question = self.df.iloc[idx]['question']
        passage = self.df.iloc[idx]['passage']

        q_encoding = self.tokenizer(question, truncation=True, padding='max_length',
                                    max_length=self.max_length, return_tensors='pt')
        p_encoding = self.tokenizer(passage, truncation=True, padding='max_length',
                                    max_length=self.max_length, return_tensors='pt')

        return {
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'p_input_ids': p_encoding['input_ids'].squeeze(0),
            'p_attention_mask': p_encoding['attention_mask'].squeeze(0)
        }

# Create the dataset and dataloader.
dataset = BiQADataset(df_all, tokenizer, max_length=128)
from torch.utils.data import DataLoader
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

import torch
import torch.nn as nn
from transformers import AutoModel

class DualEncoder(nn.Module):
    def __init__(self, model_name):
        super(DualEncoder, self).__init__()
        # Define two encoders (they can be separate or share weights)
        self.question_encoder = AutoModel.from_pretrained(model_name)
        self.passage_encoder = AutoModel.from_pretrained(model_name)
        # Mean pooling: Compute average of token embeddings (ignoring padded positions)
        self.pooling = lambda token_embeddings, attention_mask: (token_embeddings * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1, keepdim=True)

    def encode_question(self, input_ids, attention_mask):
        outputs = self.question_encoder(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = outputs.last_hidden_state
        question_embedding = self.pooling(token_embeddings, attention_mask)
        return question_embedding

    def encode_passage(self, input_ids, attention_mask):
        outputs = self.passage_encoder(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = outputs.last_hidden_state
        passage_embedding = self.pooling(token_embeddings, attention_mask)
        return passage_embedding

    def forward(self, q_input_ids, q_attention_mask, p_input_ids, p_attention_mask):
        q_embed = self.encode_question(q_input_ids, q_attention_mask)
        p_embed = self.encode_passage(p_input_ids, p_attention_mask)

        # Normalize embeddings and compute similarity via matrix multiplication
        q_norm = nn.functional.normalize(q_embed, p=2, dim=1)
        p_norm = nn.functional.normalize(p_embed, p=2, dim=1)
        similarity = torch.matmul(q_norm, p_norm.transpose(0, 1))
        return similarity

# Initialize the model.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = DualEncoder(model_name).to(device)

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel

class DualEncoder(nn.Module):
    def __init__(self, model_name):
        super(DualEncoder, self).__init__()
        # Two separate encoders can be used; for simplicity, we initialize the same model.
        self.question_encoder = AutoModel.from_pretrained(model_name)
        self.passage_encoder = AutoModel.from_pretrained(model_name)

        # Optionally add a pooling layer; here, we use mean pooling over token embeddings.
        self.pooling = lambda token_embeddings, attention_mask: (token_embeddings * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1, keepdim=True)

    def encode_question(self, input_ids, attention_mask):
        outputs = self.question_encoder(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = outputs.last_hidden_state
        question_embedding = self.pooling(token_embeddings, attention_mask)
        return question_embedding

    def encode_passage(self, input_ids, attention_mask):
        outputs = self.passage_encoder(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = outputs.last_hidden_state
        passage_embedding = self.pooling(token_embeddings, attention_mask)
        return passage_embedding

    def forward(self, q_input_ids, q_attention_mask, p_input_ids, p_attention_mask):
        q_embed = self.encode_question(q_input_ids, q_attention_mask)
        p_embed = self.encode_passage(p_input_ids, p_attention_mask)

        # Compute cosine similarity between question and passage embeddings
        # Normalize embeddings first.
        q_norm = nn.functional.normalize(q_embed, p=2, dim=1)
        p_norm = nn.functional.normalize(p_embed, p=2, dim=1)
        similarity = torch.matmul(q_norm, p_norm.transpose(0, 1))
        return similarity

from torch.utils.data import Dataset, DataLoader

class BiQADataset(Dataset):
    def __init__(self, df, tokenizer, max_length=128):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Adjust the column names to match your CSV file.
        question = self.df.iloc[idx]['question']
        passage = self.df.iloc[idx]['passage']

        q_encoding = self.tokenizer(question, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        p_encoding = self.tokenizer(passage, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')

        return {
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'p_input_ids': p_encoding['input_ids'].squeeze(0),
            'p_attention_mask': p_encoding['attention_mask'].squeeze(0)
        }

# Example usage:
model_name = 'dmis-lab/biobert-base-cased-v1.1'
tokenizer = AutoTokenizer.from_pretrained(model_name)
dataset = BiQADataset(df_all, tokenizer)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

!pip install faiss-cpu

class BiQADataset(Dataset):
    def __init__(self, df, tokenizer, max_length=128):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Adjust the column names to match your CSV file.
        # The original code assumed a column named 'question'.
        # Change 'question_text' and 'passage_text' to the actual column names in your CSV.
        question = self.df.iloc[idx]['question_text']
        passage = self.df.iloc[idx]['passage_text']

        q_encoding = self.tokenizer(question, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')
        p_encoding = self.tokenizer(passage, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')

        return {
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'p_input_ids': p_encoding['input_ids'].squeeze(0),
            'p_attention_mask': p_encoding['attention_mask'].squeeze(0)
        }

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
import faiss

# ... (Your DualEncoder class and BiQADataset class from previous cells) ...

# Load the model and tokenizer
model_name = 'dmis-lab/biobert-base-cased-v1.1'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = DualEncoder(model_name)  # Assuming you have your model defined

# Example query (replace with your actual query)
query = "What is the function of mitochondria?"

# Tokenize the query
query_inputs = tokenizer(query, return_tensors='pt')

# Get the query embedding using the model
with torch.no_grad():
    query_embedding = model.encode_question(query_inputs['input_ids'], query_inputs['attention_mask'])

# Convert query embedding to NumPy and reshape
query_embedding = query_embedding.cpu().numpy().reshape(1, -1)

# ... (Your FAISS index creation code) ...

k = 10  # Number of nearest neighbors to retrieve
distances, indices = index.search(query_embedding, k)

print("Top-k indices:", indices)
print("Distances:", distances)