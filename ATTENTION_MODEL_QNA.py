# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b2q9WShTAc7CMm1JSPQikY9baxLwAPIE
"""

# Cell 1: Setup and Dependencies
import numpy as np
import pandas as pd
import tensorflow as tf
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
from sklearn.model_selection import train_test_split
import re
import json
import requests
import os
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import nltk
from google.colab import drive

# Download necessary NLTK resources
nltk.download('punkt')

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def download_biomedical_qa_dataset():
    print("Downloading dataset from GitHub repository...")

    # Create a directory for the dataset
    os.makedirs('data', exist_ok=True)

    # Try to download BioASQ dataset first (more focused on biomedical QA)
    dataset_url = "https://raw.githubusercontent.com/Andy-jqa/biomedical-qa-datasets/main/BioASQ/BioASQ-train-factoid-4b.json"

    try:
        # Download the dataset
        response = requests.get(dataset_url)

        if response.status_code == 200:
            # Save the dataset
            with open('data/bioasq.json', 'wb') as f:
                f.write(response.content)
            print("BioASQ dataset downloaded successfully!")

            # Load the dataset
            with open('data/bioasq.json', 'r') as f:
                data = json.load(f)

            return data, "bioasq"

        else:
            print(f"Failed to download BioASQ dataset. Status code: {response.status_code}")
            # Fall back to DrugEHRQA

    except Exception as e:
        print(f"Error downloading BioASQ dataset: {e}")

    # Try DrugEHRQA as a backup
    try:
        drugqa_url = "https://raw.githubusercontent.com/Andy-jqa/biomedical-qa-datasets/main/DrugEHRQA/sample_data/dev.json"
        response = requests.get(drugqa_url)

        if response.status_code == 200:
            with open('data/drugqa.json', 'wb') as f:
                f.write(response.content)
            print("DrugQA dataset downloaded successfully!")

            # Load the dataset
            with open('data/drugqa.json', 'r') as f:
                data = json.load(f)

            return data, "drugqa"
        else:
            print(f"Failed to download DrugQA dataset. Status code: {response.status_code}")
    except Exception as e:
        print(f"Error downloading DrugQA dataset: {e}")

    # If all else fails, generate synthetic data
    print("Generating synthetic drug QA dataset as fallback...")
    return generate_synthetic_dataset(1000), "synthetic"

# Process BioASQ dataset
def process_bioasq_dataset(data):
    print("Processing BioASQ dataset...")
    questions = []
    contexts = []
    answers = []

    # Extract questions and answers
    for item in data['questions']:
        question_text = item['body']

        # BioASQ often has snippets that can be used as context
        if 'snippets' in item and len(item['snippets']) > 0:
            for snippet in item['snippets']:
                context = snippet.get('text', '')

                # Ensure we have a valid exact answer
                if 'exact_answer' in item and item['exact_answer']:
                    # Handle different formats of exact_answer
                    if isinstance(item['exact_answer'], list):
                        if len(item['exact_answer']) > 0:
                            if isinstance(item['exact_answer'][0], list):
                                answer_text = item['exact_answer'][0][0] if item['exact_answer'][0] else ""
                            else:
                                answer_text = item['exact_answer'][0]
                        else:
                            answer_text = ""
                    else:
                        answer_text = item['exact_answer']

                    # Only add if the answer appears in the context
                    if answer_text and answer_text in context:
                        questions.append(question_text)
                        contexts.append(context)
                        answers.append(answer_text)

    # Create DataFrame
    df = pd.DataFrame({
        'question': questions,
        'context': contexts,
        'answer': answers
    })

    return df

# Process DrugEHRQA dataset
def process_drugqa_dataset(data):
    print("Processing DrugEHRQA dataset...")
    questions = []
    contexts = []
    answers = []

    for entry in data:
        if 'question' in entry and 'answer' in entry:
            question = entry['question']
            answer = entry['answer']

            # For DrugEHRQA, we'll use the patient note as context if available
            if 'note' in entry and entry['note']:
                context = entry['note']
            else:
                # If no note, we'll use a default context that includes the answer
                context = f"The patient was prescribed medication. {answer}"

            questions.append(question)
            contexts.append(context)
            answers.append(answer)

    # Create DataFrame
    df = pd.DataFrame({
        'question': questions,
        'context': contexts,
        'answer': answers
    })

    return df

# Generate synthetic dataset as a fallback
def generate_synthetic_dataset(n_samples=1000):
    print("Generating synthetic drug QA dataset...")
    drugs = ["Aspirin", "Ibuprofen", "Paracetamol", "Amoxicillin", "Lisinopril",
             "Metformin", "Simvastatin", "Atorvastatin", "Omeprazole", "Amlodipine"]

    side_effects = ["headache", "nausea", "dizziness", "drowsiness", "rash",
                   "stomach pain", "dry mouth", "fatigue", "insomnia", "diarrhea"]

    uses = ["pain relief", "fever reduction", "inflammation reduction", "bacterial infections",
            "blood pressure control", "diabetes management", "cholesterol control",
            "acid reflux", "heartburn", "hypertension"]

    dosages = ["once daily", "twice daily", "three times daily", "every 4-6 hours",
               "with meals", "before meals", "after meals", "at bedtime"]

    questions = []
    contexts = []
    answers = []

    for _ in range(n_samples):
        drug = np.random.choice(drugs)
        rand_type = np.random.choice(["side_effect", "use", "dosage", "general"])

        if rand_type == "side_effect":
            effect = np.random.choice(side_effects)
            question = f"What are the side effects of {drug}?"

            effects = np.random.choice(side_effects, size=np.random.randint(2, 5), replace=False)
            effects_list = ", ".join(effects)
            answer = effects_list
            context = f"{drug} is a medication that is commonly used. {drug} may cause the following side effects: {effects_list}. Patients should consult their doctor if side effects persist."

        elif rand_type == "use":
            question = f"What is {drug} used for?"

            drug_uses = np.random.choice(uses, size=np.random.randint(1, 3), replace=False)
            uses_list = " and ".join(drug_uses)
            answer = uses_list
            context = f"{drug} is a medication commonly prescribed for {uses_list}. It works by affecting certain chemicals in the body to provide relief or treatment."

        elif rand_type == "dosage":
            dosage = np.random.choice(dosages)
            question = f"How should I take {drug}?"

            answer = dosage
            context = f"The typical dosage for {drug} is {dosage}. Always follow your doctor's recommendations. {drug} is a medication used to treat various conditions."

        else:  # general questions
            question = f"What is {drug}?"

            answer = f"a medication for various conditions"
            context = f"{drug} is a medication used to treat various conditions including some forms of pain and inflammation. It belongs to a class of drugs known as non-steroidal anti-inflammatory drugs (NSAIDs)."

        questions.append(question)
        contexts.append(context)
        answers.append(answer)

    # Create a dictionary similar to other datasets
    return {'questions': questions, 'contexts': contexts, 'answers': answers}

# Cell 3: Download and Process Dataset

# Download and process dataset
data, data_type = download_biomedical_qa_dataset()

# Process the appropriate dataset type
if data_type == "bioasq":
    df = process_bioasq_dataset(data)
elif data_type == "drugqa":
    df = process_drugqa_dataset(data)
else:  # synthetic
    # Convert the synthetic format to DataFrame
    df = pd.DataFrame({
        'question': data['questions'],
        'context': data['contexts'],
        'answer': data['answers']
    })

# Display dataset info
print(f"\nDataset shape: {df.shape}")
print("\nSample data:")
print(df.head())

# Check for missing values and clean up
print("\nChecking for missing values:")
print(df.isnull().sum())

# Fill any NaN values
df = df.fillna("")

# Ensure answers are in the context
def verify_answer_in_context(row):
    return row['answer'] in row['context']

# Check how many answers are in the context
answer_in_context = df.apply(verify_answer_in_context, axis=1)
print(f"\nAnswers found in context: {sum(answer_in_context)} out of {len(df)} samples ({sum(answer_in_context)/len(df)*100:.1f}%)")

# Filter to only keep examples where the answer is in the context
df_filtered = df[answer_in_context]
if len(df_filtered) > 100:  # Ensure we have enough data
    df = df_filtered
    print(f"Filtered dataset shape: {df.shape}")
else:
    print("Too few samples with answers in context, using the full dataset")

# Split data into train and test sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
print(f"Train set: {train_df.shape}, Test set: {test_df.shape}")

# Cell 4: Dataset Class and Model Loading

# Load BioBERT model and tokenizer
print("\nLoading BioBERT model and tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModelForQuestionAnswering.from_pretrained("dmis-lab/biobert-v1.1").to(device)

# Create a QA dataset class
class QuestionAnsweringDataset(Dataset):
    def __init__(self, questions, contexts, answers, tokenizer, max_length=384):
        self.questions = questions
        self.contexts = contexts
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question = self.questions[idx]
        context = self.contexts[idx]
        answer = self.answers[idx]

        # Find the start and end positions of the answer in the context
        answer_start = context.find(answer)
        answer_end = answer_start + len(answer) - 1 if answer_start != -1 else -1

        # Tokenize
        encoding = self.tokenizer(
            question,
            context,
            truncation="only_second",
            max_length=self.max_length,
            stride=128,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
            return_tensors="pt"
        )

        # If the answer is not found in the context, set start/end positions to 0
        if answer_start == -1:
            start_positions = torch.tensor([0])
            end_positions = torch.tensor([0])
        else:
            # Convert answer character positions to token positions
            offsets = encoding["offset_mapping"][0]

            # Find which token contains the start of the answer
            start_token = 0
            while start_token < len(offsets) and (offsets[start_token][0] > answer_start or offsets[start_token][1] <= answer_start):
                start_token += 1

            # Find which token contains the end of the answer
            end_token = start_token
            answer_end_position = answer_start + len(answer) - 1
            while end_token < len(offsets) and (offsets[end_token][0] > answer_end_position or offsets[end_token][1] <= answer_end_position):
                end_token += 1

            # If we didn't find valid tokens, default to 0
            if start_token >= len(offsets) or end_token >= len(offsets):
                start_positions = torch.tensor([0])
                end_positions = torch.tensor([0])
            else:
                start_positions = torch.tensor([start_token])
                end_positions = torch.tensor([end_token])

        return {
            "input_ids": encoding["input_ids"][0],
            "attention_mask": encoding["attention_mask"][0],
            "start_positions": start_positions,
            "end_positions": end_positions,
            "offset_mapping": encoding["offset_mapping"][0]
        }

# Create datasets
train_dataset = QuestionAnsweringDataset(
    train_df['question'].tolist(),
    train_df['context'].tolist(),
    train_df['answer'].tolist(),
    tokenizer
)

test_dataset = QuestionAnsweringDataset(
    test_df['question'].tolist(),
    test_df['context'].tolist(),
    test_df['answer'].tolist(),
    tokenizer
)

# Create data loaders
batch_size = 8
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

# Cell 5: Model Training

# Set up training parameters
learning_rate = 5e-5
epochs = 3
warmup_steps = 500
weight_decay = 0.01

# Set up optimizer and scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
total_steps = len(train_dataloader) * epochs
scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=total_steps)

# Training loop
print("\nStarting training...")
model.train()
for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")
    epoch_loss = 0

    progress_bar = tqdm(train_dataloader, desc=f"Training epoch {epoch+1}")
    for batch in progress_bar:
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        start_positions = batch["start_positions"].to(device)
        end_positions = batch["end_positions"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            start_positions=start_positions,
            end_positions=end_positions
        )

        loss = outputs.loss
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        epoch_loss += loss.item()
        progress_bar.set_postfix({"loss": loss.item()})

    avg_epoch_loss = epoch_loss / len(train_dataloader)
    print(f"Average training loss: {avg_epoch_loss:.4f}")

# Save the fine-tuned model
model_save_path = "./biobert_drug_qa"
os.makedirs(model_save_path, exist_ok=True)
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"Model saved to {model_save_path}")

# Cell 6: Model Evaluation

# Evaluation
print("\nEvaluating model on test set...")
model.eval()
exact_matches = 0
f1_scores = []

def compute_f1(prediction, ground_truth):
    """Calculate F1 score between prediction and ground truth"""
    prediction_tokens = prediction.lower().split()
    ground_truth_tokens = ground_truth.lower().split()

    common = set(prediction_tokens) & set(ground_truth_tokens)

    if len(common) == 0:
        return 0

    precision = len(common) / len(prediction_tokens)
    recall = len(common) / len(ground_truth_tokens)

    f1 = 2 * precision * recall / (precision + recall)
    return f1

def evaluate_exact_match(prediction, ground_truth):
    """Check if prediction exactly matches ground truth"""
    return prediction.lower() == ground_truth.lower()

# Create a list to map batch samples back to test_df indices
test_indices = list(range(len(test_df)))

with torch.no_grad():
    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="Evaluating")):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        for i in range(input_ids.shape[0]):
            # Get start and end logits
            start_logits = outputs.start_logits[i].detach().cpu().numpy()
            end_logits = outputs.end_logits[i].detach().cpu().numpy()

            # Get the most likely beginning and end of the answer
            start_idx = np.argmax(start_logits)
            end_idx = np.argmax(end_logits)

            # Convert to tokens
            answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][start_idx:end_idx+1])

            # Remove special tokens and construct the full answer
            answer = tokenizer.convert_tokens_to_string(answer_tokens)

            # Calculate original index in test_df
            batch_position = batch_idx * batch_size + i
            if batch_position < len(test_indices):
                orig_idx = test_indices[batch_position]

                if orig_idx < len(test_df):
                    true_answer = test_df['answer'].iloc[orig_idx]

                    # Calculate metrics
                    if evaluate_exact_match(answer, true_answer):
                        exact_matches += 1

                    f1 = compute_f1(answer, true_answer)
                    f1_scores.append(f1)

# Calculate final metrics
exact_match_score = 100 * exact_matches / len(test_df)
f1_score = 100 * sum(f1_scores) / max(len(f1_scores), 1)  # Avoid division by zero

print(f"Exact Match: {exact_match_score:.2f}%")
print(f"F1 Score: {f1_score:.2f}%")

# Cell 7: Sample Question Testing

# Create a question answering pipeline with our fine-tuned model
nlp = pipeline("question-answering", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

# Test with some drug-related questions
test_contexts = [
    """Aspirin is a nonsteroidal anti-inflammatory drug (NSAID) used to reduce pain, fever, and inflammation.
    Common side effects include stomach irritation, nausea, vomiting, and heartburn. It should be taken with food
    to minimize gastrointestinal side effects. The typical dosage for adults is 325-650 mg every 4 hours as needed.""",

    """Metformin is an oral diabetes medicine that helps control blood sugar levels. It is used together with diet
    and exercise to improve blood sugar control in adults with type 2 diabetes. Common side effects include diarrhea,
    nausea, and stomach upset. The recommended starting dose is 500 mg twice daily with meals.""",

    """Antibiotics are medicines that fight bacterial infections in people and animals. They work by killing the
    bacteria or by making it hard for the bacteria to grow and multiply. Antibiotics only work against bacteria,
    not viruses. They're ineffective against viral infections like the common cold, flu, most sore throats,
    bronchitis, and many sinus and ear infections."""
]

test_questions = [
    "What are the side effects of Aspirin?",
    "How should I take Metformin?",
    "Can antibiotics treat viral infections?"
]

print("\nTesting the model with sample questions:")
for i in range(len(test_questions)):
    result = nlp(question=test_questions[i], context=test_contexts[i])
    print(f"Q: {test_questions[i]}")
    print(f"A: {result['answer']}")
    print(f"Score: {result['score']:.4f}")
    print()

# Cell 8: Interactive Drug QA Function

# Function to get answer for a user question
def ask_drug_question(question, context):
    """Function to get answers from the BioBERT model"""
    # Make sure we're in evaluation mode
    model.eval()

    # Tokenize the input
    inputs = tokenizer(
        question,
        context,
        return_tensors="pt",
        max_length=384,
        truncation="only_second",
        padding="max_length"
    )

    # Move to the same device as the model
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Get model prediction
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    # Get the most likely beginning and end of answer
    answer_start = torch.argmax(outputs.start_logits)
    answer_end = torch.argmax(outputs.end_logits)

    # Convert to tokens and then to string
    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end+1])
    answer = tokenizer.convert_tokens_to_string(answer_tokens)

    # Calculate confidence score (softmax of logits)
    start_score = torch.nn.functional.softmax(outputs.start_logits, dim=1).max().item()
    end_score = torch.nn.functional.softmax(outputs.end_logits, dim=1).max().item()
    confidence = (start_score + end_score) / 2

    return answer, confidence

# Complete default context with information about various drugs
default_context = """
Aspirin is a nonsteroidal anti-inflammatory drug (NSAID) used to reduce pain, fever, and inflammation.
Common side effects include stomach irritation, nausea, vomiting, and heartburn. It should be taken with food
to minimize gastrointestinal side effects. The typical dosage for adults is 325-650 mg every 4 hours as needed.

Metformin is an oral diabetes medicine that helps control blood sugar levels. It is often used to treat type 2 diabetes.
Common side effects include nausea, vomiting, stomach upset, diarrhea, and a metallic taste in the mouth.
The typical dosage starts at 500 mg twice daily with meals.

Lisinopril is an ACE inhibitor used to treat high blood pressure and heart failure.
Common side effects include dry cough, dizziness, headache, and fatigue.
The typical dosage ranges from 10-40 mg once daily.

Amoxicillin is an antibiotic used to treat bacterial infections.
Common side effects include diarrhea, rash, nausea, and vomiting.
The typical dosage for adults is 250-500 mg every 8 hours or 500-875 mg every 12 hours.
"""

# Function to interactively ask questions
def interactive_qa():
    print("Drug Information Question-Answering System")
    print("Enter 'quit' to exit")
    print("-" * 50)

    while True:
        # Get user question
        user_question = input("\nEnter your question about medications: ")

        if user_question.lower() in ['quit', 'exit', 'q']:
            print("Exiting the QA system. Goodbye!")
            break

        # Use default context or let user provide one
        use_default = input("Use default drug information context? (y/n): ").lower() == 'y'

        if use_default:
            context = default_context
        else:
            print("Enter your own context (medical information):")
            context = input()

        # Get answer
        try:
            answer, confidence = ask_drug_question(user_question, context)

            print("\nAnswer:", answer)
            print(f"Confidence: {confidence:.2%}")

            if confidence < 0.5:
                print("Note: The model has low confidence in this answer.")
        except Exception as e:
            print(f"Error processing question: {e}")

# Run the interactive system
interactive_qa()